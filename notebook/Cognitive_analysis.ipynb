{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43mV8fT0LRMX"
      },
      "source": [
        "# **MemoTag AI/ML Task: Cognitive Stress Detection Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvQwipvyAClD"
      },
      "source": [
        "## Install and import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvWWhHYf_gJw"
      },
      "outputs": [],
      "source": [
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twHMYdei_Til"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import speech_recognition as sr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLrA35nVAcTO"
      },
      "source": [
        "## Directory for audio files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDo7zO99_kXe"
      },
      "outputs": [],
      "source": [
        "# Set up directory for audio files\n",
        "AUDIO_DIR = \"audio_samples\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CxR7dYlAR7d"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VleHNY8lAZ00"
      },
      "outputs": [],
      "source": [
        "audio_features = {}\n",
        "feature_matrix = None\n",
        "feature_names = []\n",
        "normalized_features = None\n",
        "cluster_results = None\n",
        "speech_recognizer = sr.Recognizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgMA6QEPAimm"
      },
      "source": [
        "### Load audio files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mCgGsElAlS4"
      },
      "outputs": [],
      "source": [
        "def load_audio_file(file_path):\n",
        "    \"\"\"Load an audio file using librosa.\"\"\"\n",
        "    try:\n",
        "        audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
        "        return audio_data, sample_rate\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {str(e)}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dh1bUj2Aofu"
      },
      "source": [
        "### Function to convert speech to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqqm5je1Ansd"
      },
      "outputs": [],
      "source": [
        "def speech_to_text(audio_file):\n",
        "    \"\"\"Convert speech in audio file to text using Google's API.\"\"\"\n",
        "    try:\n",
        "        with sr.AudioFile(audio_file) as source:\n",
        "            audio = speech_recognizer.record(source)\n",
        "            text = speech_recognizer.recognize_google(audio)\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Speech recognition failed: {str(e)}\")\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBtjLdxABCsG"
      },
      "source": [
        "### Feature extraction functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JUgClkrAt_8"
      },
      "outputs": [],
      "source": [
        "def extract_audio_features(audio, sample_rate, file_id):\n",
        "    \"\"\"Extract various acoustic features from audio.\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Basic audio properties\n",
        "    features['duration'] = librosa.get_duration(y=audio, sr=sample_rate)\n",
        "\n",
        "    # Pitch analysis\n",
        "    pitches = librosa.piptrack(y=audio, sr=sample_rate)[0]\n",
        "    pitches = pitches[pitches > 0]  # Remove zero pitches\n",
        "    if len(pitches) > 0:\n",
        "        features['pitch_mean'] = np.mean(pitches)\n",
        "        features['pitch_std'] = np.std(pitches)\n",
        "    else:\n",
        "        features['pitch_mean'] = features['pitch_std'] = 0\n",
        "\n",
        "    # Speech rate and pauses\n",
        "    zero_crossings = librosa.zero_crossings(audio)\n",
        "    features['speech_rate'] = sum(zero_crossings) / len(audio)\n",
        "\n",
        "    # Energy and pauses\n",
        "    rms_energy = librosa.feature.rms(y=audio)[0]\n",
        "    features['energy_mean'] = np.mean(rms_energy)\n",
        "\n",
        "    # Spectral features\n",
        "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)[0]\n",
        "    features['spectral_centroid'] = np.mean(spectral_centroid)\n",
        "\n",
        "    # MFCCs (commonly used in speech analysis)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
        "    for i in range(13):\n",
        "        features[f'mfcc_{i+1}'] = np.mean(mfccs[i])\n",
        "\n",
        "    # Store features\n",
        "    audio_features[file_id] = features\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWjAggEuBOPK"
      },
      "source": [
        "### Speech text content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXV-jrYPBHHk"
      },
      "outputs": [],
      "source": [
        "def analyze_text_content(text, file_id):\n",
        "    \"\"\"Analyze linguistic features of transcribed text.\"\"\"\n",
        "    text_features = {}\n",
        "\n",
        "    if not text:\n",
        "        # Default values if no text was recognized\n",
        "        text_features['word_count'] = 0\n",
        "        text_features['unique_word_ratio'] = 0\n",
        "        return text_features\n",
        "\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n",
        "\n",
        "    # Basic text statistics\n",
        "    text_features['word_count'] = len(words)\n",
        "    text_features['sentence_count'] = len(sentences)\n",
        "\n",
        "    # Vocabulary richness\n",
        "    unique_words = set(words)\n",
        "    text_features['unique_word_ratio'] = len(unique_words) / len(words) if words else 0\n",
        "\n",
        "    # Speech disfluencies\n",
        "    hesitation_words = ['um', 'uh', 'ah', 'like', 'you know']\n",
        "    text_features['hesitation_count'] = sum(1 for word in words if word in hesitation_words)\n",
        "\n",
        "    # Store combined features\n",
        "    if file_id in audio_features:\n",
        "        audio_features[file_id].update(text_features)\n",
        "\n",
        "    return text_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl56Irn-BV2L"
      },
      "source": [
        "### Processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cx5PkvFBW4h"
      },
      "outputs": [],
      "source": [
        "def process_audio_file(file_path):\n",
        "    \"\"\"Full processing pipeline for a single audio file.\"\"\"\n",
        "    filename = os.path.basename(file_path)\n",
        "    print(f\"\\nProcessing {filename}...\")\n",
        "\n",
        "    # Step 1: Load audio\n",
        "    audio, sample_rate = load_audio_file(file_path)\n",
        "    if audio is None:\n",
        "        return None\n",
        "\n",
        "    # Step 2: Extract audio features\n",
        "    acoustic_features = extract_audio_features(audio, sample_rate, filename)\n",
        "\n",
        "    # Step 3: Speech recognition\n",
        "    text = speech_to_text(file_path)\n",
        "    print(f\"Transcribed text: {text[:100]}...\" if len(text) > 100 else f\"Transcribed text: {text}\")\n",
        "\n",
        "    # Step 4: Text analysis\n",
        "    linguistic_features = analyze_text_content(text, filename)\n",
        "\n",
        "    return {**acoustic_features, **linguistic_features}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYwHIDP_BbFm"
      },
      "source": [
        "### Text analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQUy6VqDBlUw"
      },
      "outputs": [],
      "source": [
        "def run_analysis(audio_files=None):\n",
        "    \"\"\"Main function to run the complete analysis.\"\"\"\n",
        "    global feature_matrix, feature_names\n",
        "\n",
        "    if audio_files is None:\n",
        "        audio_files = [\n",
        "            os.path.join(AUDIO_DIR, f)\n",
        "            for f in os.listdir(AUDIO_DIR)\n",
        "            if f.lower().endswith(('.wav', '.mp3'))\n",
        "        ]\n",
        "\n",
        "    print(f\"Found {len(audio_files)} audio files to analyze\")\n",
        "\n",
        "    # Process each file\n",
        "    for file_path in audio_files:\n",
        "        process_audio_file(file_path)\n",
        "\n",
        "    # Convert features to DataFrame\n",
        "    if audio_features:\n",
        "        df = pd.DataFrame.from_dict(audio_features, orient='index')\n",
        "        feature_names = df.columns.tolist()\n",
        "        feature_matrix = df.values\n",
        "        return df\n",
        "    else:\n",
        "        print(\"No features were extracted\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHJxk1m4BnyH"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1WhuBN7BouR"
      },
      "outputs": [],
      "source": [
        "def clean_and_normalize_data():\n",
        "    \"\"\"Handle missing values and normalize features.\"\"\"\n",
        "    global feature_matrix, normalized_features\n",
        "\n",
        "    # Handle missing values\n",
        "    if np.isnan(feature_matrix).any():\n",
        "        from sklearn.impute import SimpleImputer\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        feature_matrix = imputer.fit_transform(feature_matrix)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    normalized_features = scaler.fit_transform(feature_matrix)\n",
        "    return normalized_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18-Jd85ByII"
      },
      "source": [
        "### Visualization functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1vYSg_CBqFy"
      },
      "outputs": [],
      "source": [
        "def plot_feature_clusters(pca_results, labels):\n",
        "    \"\"\"Visualize clusters in 2D PCA space.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(pca_results[:, 0], pca_results[:, 1], c=labels, cmap='viridis')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.title(\"Speech Feature Clusters\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1CFHbGDDsER"
      },
      "source": [
        " ### Identify abnormal speech samples by ML/NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_hhGFNKD1zR"
      },
      "outputs": [],
      "source": [
        "def run_risk_analysis():\n",
        "    \"\"\"\n",
        "    Perform risk analysis on speech samples to identify abnormal patterns\n",
        "    that might indicate cognitive or speech disorders.\n",
        "    \"\"\"\n",
        "    global normalized_features, feature_names\n",
        "\n",
        "    print(\"\\n--- Risk Analysis Results ---\")\n",
        "\n",
        "    if normalized_features is None or len(normalized_features) < 3:\n",
        "        print(\"Insufficient data for meaningful risk analysis\")\n",
        "        return\n",
        "\n",
        "    # Method 1: Isolation Forest for anomaly detection\n",
        "    print(\"Performing anomaly detection...\")\n",
        "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "    anomaly_scores = iso_forest.fit_predict(normalized_features)\n",
        "\n",
        "    anomalies = np.where(anomaly_scores == -1)[0]\n",
        "\n",
        "    # Method 2: Statistical outlier detection using z-scores\n",
        "    z_scores = np.abs(scipy.stats.zscore(normalized_features))\n",
        "    outliers = np.where(np.any(z_scores > 3, axis=1))[0]\n",
        "\n",
        "    potential_risks = list(set(anomalies) | set(outliers))\n",
        "\n",
        "    feature_df = pd.DataFrame(feature_matrix, columns=feature_names)\n",
        "    feature_df.index = list(audio_features.keys())\n",
        "\n",
        "    # Display risk results\n",
        "    if potential_risks:\n",
        "        print(f\"\\nFound {len(potential_risks)} potentially abnormal speech samples:\")\n",
        "        for idx in potential_risks:\n",
        "            sample_id = feature_df.index[idx]\n",
        "            print(f\"- Sample {sample_id}\")\n",
        "\n",
        "\n",
        "            sample_features = normalized_features[idx]\n",
        "            feature_df_norm = pd.DataFrame(normalized_features, columns=feature_names)\n",
        "            feature_df_norm.index = feature_df.index\n",
        "\n",
        "            # Find most deviant features for this sample\n",
        "            deviations = abs(sample_features - np.mean(normalized_features, axis=0))\n",
        "            most_deviant = np.argsort(deviations)[-3:]  # Top 3 most deviant features\n",
        "\n",
        "            print(\"  Notable deviations:\")\n",
        "            for feat_idx in most_deviant:\n",
        "                feat_name = feature_names[feat_idx]\n",
        "                raw_value = feature_df.iloc[idx][feat_name]\n",
        "                z_score = (raw_value - feature_df[feat_name].mean()) / feature_df[feat_name].std() if feature_df[feat_name].std() != 0 else 0\n",
        "                direction = \"high\" if z_score > 0 else \"low\"\n",
        "\n",
        "                print(f\"  * {feat_name}: {raw_value:.2f} ({abs(z_score):.2f} std. {direction})\")\n",
        "\n",
        "        # Visualize the potentially risky samples\n",
        "        plot_risk_visualization(normalized_features, potential_risks)\n",
        "    else:\n",
        "        print(\"No abnormal speech patterns detected in the samples\")\n",
        "\n",
        "    return potential_risks\n",
        "\n",
        "def plot_risk_visualization(features, risk_indices):\n",
        "    \"\"\"\n",
        "    Create visualizations to highlight potentially risky samples.\n",
        "    \"\"\"\n",
        "    # 1. Perform PCA for dimensionality reduction\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_results = pca.fit_transform(features)\n",
        "\n",
        "    # 2. Create a risk visualization plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot normal samples\n",
        "    normal_indices = [i for i in range(len(features)) if i not in risk_indices]\n",
        "    plt.scatter(\n",
        "        pca_results[normal_indices, 0],\n",
        "        pca_results[normal_indices, 1],\n",
        "        c='blue',\n",
        "        label='Normal',\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "    # Plot risky samples\n",
        "    plt.scatter(\n",
        "        pca_results[risk_indices, 0],\n",
        "        pca_results[risk_indices, 1],\n",
        "        c='red',\n",
        "        marker='X',\n",
        "        s=100,\n",
        "        label='Potential Risk',\n",
        "        alpha=0.9\n",
        "    )\n",
        "\n",
        "    # Add sample labels for risky samples\n",
        "    for idx in risk_indices:\n",
        "        sample_id = list(audio_features.keys())[idx]\n",
        "        plt.annotate(\n",
        "            sample_id,\n",
        "            (pca_results[idx, 0], pca_results[idx, 1]),\n",
        "            xytext=(5, 5),\n",
        "            textcoords='offset points',\n",
        "            fontsize=9\n",
        "        )\n",
        "\n",
        "    plt.xlabel(f\"Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)\")\n",
        "    plt.ylabel(f\"Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)\")\n",
        "    plt.title(\"Speech Analysis Risk Assessment\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Create heatmap of feature deviations for risky samples\n",
        "    if len(risk_indices) > 0:\n",
        "        plt.figure(figsize=(14, len(risk_indices) * 0.8 + 3))\n",
        "\n",
        "        # Calculate z-scores\n",
        "        z_data = scipy.stats.zscore(features)[risk_indices]\n",
        "\n",
        "        # Heatmap\n",
        "        sample_labels = [list(audio_features.keys())[idx] for idx in risk_indices]\n",
        "        sns.heatmap(\n",
        "            z_data,\n",
        "            cmap='coolwarm',\n",
        "            yticklabels=sample_labels,\n",
        "            xticklabels=feature_names,\n",
        "            center=0,\n",
        "            vmin=-3,\n",
        "            vmax=3,\n",
        "            annot=False,\n",
        "            fmt='.1f'\n",
        "        )\n",
        "        plt.title(\"Feature Deviation Heatmap for Potential Risk Samples\")\n",
        "        plt.xlabel(\"Features\")\n",
        "        plt.ylabel(\"Samples\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CQB2Z1xE3d-"
      },
      "outputs": [],
      "source": [
        "def get_speech_insights(features, clusters, risk_samples=None):\n",
        "    \"\"\"\n",
        "    Analyzes speech data to identify patterns and potential concerns.\n",
        "\n",
        "    Args:\n",
        "        features: DataFrame with speech features for each sample\n",
        "        clusters: Group assignments for each speech sample\n",
        "        risk_samples: List of samples flagged as potential risks\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with organized insights about the speech patterns\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    results = {\n",
        "        \"summary\": {},\n",
        "        \"clusters\": {},\n",
        "        \"risk_patterns\": {},\n",
        "        \"key_features\": {},\n",
        "        \"recommendations\": []\n",
        "    }\n",
        "\n",
        "    # Statistics\n",
        "    total_samples = len(features)\n",
        "    unique_clusters = len(np.unique(clusters))\n",
        "    high_risk_count = len(risk_samples) if risk_samples is not None else 0\n",
        "    risk_percentage = round(high_risk_count / total_samples * 100, 1) if total_samples > 0 else 0\n",
        "\n",
        "    #Stats summary\n",
        "    results[\"summary\"] = {\n",
        "        \"total_samples\": total_samples,\n",
        "        \"clusters_found\": unique_clusters,\n",
        "        \"risk_samples\": high_risk_count,\n",
        "        \"risk_percentage\": risk_percentage\n",
        "    }\n",
        "\n",
        "    for cluster_id in range(unique_clusters):\n",
        "        cluster_samples = features.iloc[clusters == cluster_id]\n",
        "        cluster_size = len(cluster_samples)\n",
        "\n",
        "        cluster_average = cluster_samples.mean()\n",
        "        overall_average = features.mean()\n",
        "\n",
        "        feature_variation = features.std()\n",
        "        standardized_differences = (cluster_average - overall_average) / feature_variation\n",
        "\n",
        "        standout_features = standardized_differences.abs().sort_values(ascending=False).head(5)\n",
        "\n",
        "        cluster_cohesion = \"N/A\"\n",
        "        if len(cluster_samples) > 1 and unique_clusters > 1:\n",
        "            try:\n",
        "                from sklearn.metrics import silhouette_samples\n",
        "                cluster_indices = np.where(clusters == cluster_id)[0]\n",
        "                cluster_cohesion = silhouette_samples(features.values, clusters)[cluster_indices].mean()\n",
        "                cluster_cohesion = round(cluster_cohesion, 3)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        results[\"clusters\"][f\"cluster_{cluster_id}\"] = {\n",
        "            \"size\": cluster_size,\n",
        "            \"percentage\": round(cluster_size / total_samples * 100, 1),\n",
        "            \"silhouette_score\": cluster_cohesion,\n",
        "            \"distinctive_features\": {\n",
        "                feature: {\n",
        "                    \"value\": round(cluster_average[feature], 2),\n",
        "                    \"diff_from_mean\": round(cluster_average[feature] - overall_average[feature], 2),\n",
        "                    \"std_diff\": round(standardized_differences[feature], 2),\n",
        "                    \"direction\": \"higher\" if standardized_differences[feature] > 0 else \"lower\"\n",
        "                }\n",
        "                for feature in standout_features.index\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "        description = \"This cluster is characterized by \"\n",
        "        feature_descriptions = []\n",
        "\n",
        "        for feature in standout_features.index[:3]:\n",
        "            direction = \"higher\" if standardized_differences[feature] > 0 else \"lower\"\n",
        "            strength = \"significantly \" if abs(standardized_differences[feature]) > 2 else \"\"\n",
        "            feature_descriptions.append(f\"{strength}{direction} {feature}\")\n",
        "\n",
        "        results[\"clusters\"][f\"cluster_{cluster_id}\"][\"interpretation\"] = description + \", \".join(feature_descriptions)\n",
        "\n",
        "    if risk_samples is not None and len(risk_samples) > 0:\n",
        "        risk_data = features.iloc[risk_samples]\n",
        "\n",
        "        risk_by_cluster = {}\n",
        "        for idx in risk_samples:\n",
        "            cluster = clusters[idx]\n",
        "            risk_by_cluster[cluster] = risk_by_cluster.get(cluster, 0) + 1\n",
        "\n",
        "        results[\"risk_patterns\"][\"cluster_distribution\"] = risk_by_cluster\n",
        "\n",
        "        risk_z_scores = (risk_data - features.mean()) / features.std()\n",
        "        mean_deviations = risk_z_scores.abs().mean().sort_values(ascending=False)\n",
        "\n",
        "        results[\"risk_patterns\"][\"common_deviations\"] = {\n",
        "            feature: round(score, 2) for feature, score in mean_deviations.head(5).items()\n",
        "        }\n",
        "\n",
        "        results[\"risk_patterns\"][\"individual_samples\"] = {}\n",
        "        for idx in risk_samples:\n",
        "            sample_id = features.index[idx]\n",
        "            sample = features.iloc[idx]\n",
        "            sample_z = (sample - features.mean()) / features.std()\n",
        "            extreme_features = sample_z.abs().sort_values(ascending=False).head(3)\n",
        "\n",
        "            results[\"risk_patterns\"][\"individual_samples\"][sample_id] = {\n",
        "                \"cluster\": int(clusters[idx]),\n",
        "                \"extreme_features\": {\n",
        "                    feature: {\n",
        "                        \"value\": round(sample[feature], 2),\n",
        "                        \"z_score\": round(sample_z[feature], 2)\n",
        "                    }\n",
        "                    for feature in extreme_features.index\n",
        "                }\n",
        "            }\n",
        "\n",
        "    # Identify which features are most important overall\n",
        "    feature_importance = {}\n",
        "    for feature in features.columns:\n",
        "        # Importance based on variation\n",
        "        feature_std = features[feature].std()\n",
        "        between_cluster_variance = np.var([\n",
        "            features.loc[clusters == c, feature].mean()\n",
        "            for c in range(unique_clusters)\n",
        "        ]) if unique_clusters > 1 else 0\n",
        "\n",
        "\n",
        "        importance = (feature_std * between_cluster_variance) if between_cluster_variance > 0 else feature_std\n",
        "        feature_importance[feature] = importance\n",
        "\n",
        "    max_importance = max(feature_importance.values()) if feature_importance else 1\n",
        "    for feature in feature_importance:\n",
        "        feature_importance[feature] /= max_importance\n",
        "\n",
        "    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
        "    results[\"key_features\"] = {\n",
        "        feature: round(score, 3) for feature, score in sorted_features[:10]\n",
        "    }\n",
        "\n",
        "    if high_risk_count > 0:\n",
        "        top_risk_features = list(results[\"risk_patterns\"][\"common_deviations\"].keys())[:3]\n",
        "        results[\"recommendations\"].append(\n",
        "            f\"Review {high_risk_count} samples identified as potential risks, particularly focusing on \"\n",
        "            f\"abnormal patterns in {', '.join(top_risk_features)}\"\n",
        "        )\n",
        "\n",
        "    if unique_clusters > 1:\n",
        "        for cluster_id in range(unique_clusters):\n",
        "            cluster_info = results[\"clusters\"][f\"cluster_{cluster_id}\"]\n",
        "            if cluster_info[\"size\"] < total_samples * 0.2:  # If cluster is small (< 20%)\n",
        "                top_feature = list(cluster_info[\"distinctive_features\"].keys())[0]\n",
        "                results[\"recommendations\"].append(\n",
        "                    f\"Investigate Cluster {cluster_id} as a minority group ({cluster_info['percentage']}% of samples) \"\n",
        "                    f\"with distinctive {top_feature} patterns\"\n",
        "                )\n",
        "\n",
        "    top_features = list(results[\"key_features\"].keys())[:3]\n",
        "    results[\"recommendations\"].append(\n",
        "        f\"Focus future analysis on the top identified features: {', '.join(top_features)}\"\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_speech_insights(insights):\n",
        "    \"\"\"\n",
        "    Displays the speech analysis insights in a readable format.\n",
        "\n",
        "    Args:\n",
        "        insights: Dictionary of insights from get_speech_insights()\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*30 + \"SPEECH ANALYSIS INSIGHTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    summary = insights[\"summary\"]\n",
        "    print(f\"\\nANALYSIS OVERVIEW:\")\n",
        "    print(f\"- {summary['total_samples']} speech samples analyzed\")\n",
        "    print(f\"- {summary['clusters_found']} distinct speech pattern clusters identified\")\n",
        "    print(f\"- {summary['risk_samples']} samples ({summary['risk_percentage']}%) flagged for potential risk\")\n",
        "\n",
        "    print(\"\\nCLUSTER ANALYSIS:\")\n",
        "    for cluster_id, cluster_info in insights[\"clusters\"].items():\n",
        "        print(f\"\\n  {cluster_id.upper()} ({cluster_info['percentage']}% of samples):\")\n",
        "        print(f\"  - {cluster_info['interpretation']}\")\n",
        "        print(f\"  - Cohesion score: {cluster_info['silhouette_score']}\")\n",
        "        print(\"  - Key features:\")\n",
        "\n",
        "        for feature, details in cluster_info[\"distinctive_features\"].items():\n",
        "            print(f\"    * {feature}: {details['value']} \"\n",
        "                  f\"({details['direction']} by {abs(details['std_diff']):.1f} std)\")\n",
        "\n",
        "    # Risk analysis section\n",
        "    if \"risk_patterns\" in insights and insights[\"risk_patterns\"]:\n",
        "        print(\"\\nRISK PATTERN ANALYSIS:\")\n",
        "\n",
        "\n",
        "        if \"cluster_distribution\" in insights[\"risk_patterns\"]:\n",
        "            print(\"  Distribution of risk samples across clusters:\")\n",
        "            for cluster, count in insights[\"risk_patterns\"][\"cluster_distribution\"].items():\n",
        "                print(f\"  - Cluster {cluster}: {count} samples\")\n",
        "\n",
        "        # Show common patterns\n",
        "        if \"common_deviations\" in insights[\"risk_patterns\"]:\n",
        "            print(\"\\n  Common feature deviations in risk samples:\")\n",
        "            for feature, score in insights[\"risk_patterns\"][\"common_deviations\"].items():\n",
        "                print(f\"  - {feature}: {score:.2f} std deviation (average)\")\n",
        "\n",
        "        # Show highest risk individual samples\n",
        "        if \"individual_samples\" in insights[\"risk_patterns\"]:\n",
        "            samples = list(insights[\"risk_patterns\"][\"individual_samples\"].items())\n",
        "            if samples:\n",
        "                print(\"\\n  Highest risk samples:\")\n",
        "                for i, (sample_id, details) in enumerate(samples[:3]):\n",
        "                    print(f\"  - {sample_id} (Cluster {details['cluster']}):\")\n",
        "                    for feature, values in details[\"extreme_features\"].items():\n",
        "                        print(f\"    * {feature}: {values['value']} (z-score: {values['z_score']:.2f})\")\n",
        "\n",
        "    # Feature importance\n",
        "    print(\"\\nKEY FEATURES IMPORTANCE:\")\n",
        "    for i, (feature, importance) in enumerate(insights[\"key_features\"].items()):\n",
        "        if i < 5:  # Show top 5\n",
        "            print(f\"  - {feature}: {importance:.3f}\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\nRECOMMENDATIONS:\")\n",
        "    for i, rec in enumerate(insights[\"recommendations\"]):\n",
        "        print(f\"  {i+1}. {rec}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdgsJOPEB1M7"
      },
      "source": [
        "### Main execution block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xqk4JmWsB3G0",
        "outputId": "a3f90f20-2324-4cd0-e940-7d3d73e22b2d"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting cognitive speech analysis...\")\n",
        "\n",
        "    # Step 1: Process all audio files\n",
        "    features_df = run_analysis()\n",
        "\n",
        "    if features_df is not None:\n",
        "        # Step 2: Preprocess data\n",
        "        clean_and_normalize_data()\n",
        "\n",
        "        # Step 3: Dimensionality reduction\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_results = pca.fit_transform(normalized_features)\n",
        "        print(f\"PCA explained variance: {pca.explained_variance_ratio_}\")\n",
        "\n",
        "        # Step 4: Clustering\n",
        "        kmeans = KMeans(n_clusters=2)\n",
        "        clusters = kmeans.fit_predict(normalized_features)\n",
        "\n",
        "        # Step 5: Visualization of clusters\n",
        "        plot_feature_clusters(pca_results, clusters)\n",
        "\n",
        "        # Step 6: Risk analysis to identify abnormal patterns\n",
        "        risk_indices = run_risk_analysis()\n",
        "\n",
        "        # Step 7: Generate and display insights\n",
        "        insights = get_speech_insights (features_df, clusters, risk_indices)\n",
        "        print_speech_insights(insights)\n",
        "\n",
        "\n",
        "        print(\"\\nAnalysis complete!\")\n",
        "    else:\n",
        "        print(\"Analysis failed - no features were extracted\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
